\textbf{Privileged Information:} The core idea of \textit{Learning Using Privileged Information (LUPI)} to learn a better classification model is that the empirical risk minimization in $X$ space can be larger than empirical risk minimization with privileged information in the joint $X \times X^*$ space \cite{pechyony2010theory}. Based on that, Vapnik et al. \cite{vapnik2015learning} proposed the $SVM+$ method that can control Student's concept of similarity between training examples in the framework of LUPI. The LUPI framework is applied in various of learning scenarios to improve the performance of the learning model, especially in computer vision.  Sharmanska et al. \cite{Sharmanska_2013_ICCV} proposed a Rank Transfer method that uses attributes, annotator
rationales, object bounding boxes, and textual descriptions as the privileged information for object recognition. Motiian et al. \cite{Motiian_2016_CVPR} proposed \textit{the information bottleneck method with privileged information (IBPI)} that leverage the auxiliary information such as like supplemental visual features, bounding box annotations and 3D skeleton tracking data to improve visual recognition performance. Li et al. \cite{Li_2016_CVPR} proposed a fast SVM+ that uses $\ell_2$ loss and can be solved efficiently with SMO algorithm.

\textbf{Distillation:} The idea of \textit{Distillation} is to use a simpler student model to simulate the behavior of a whole ensemble
of complex (teacher) models \cite{hinton2015distilling}. According to the ERM principle, if the teacher models generalize well and the student model can mimick the output of the teacher models (soft label) well on the training data, it will typically do much better than a small model trained on the same dataset without using any knowledge from the teacher models. Distillation is typically used for training the deep neural network for knowledge transfer between different models and tasks. Tzeng et al. \cite{Tzeng_2015_ICCV} proposed a CNN architecture for domain adaptation to leverage the knowledge from limited or no labeled data using the soft label. Urban et al. \cite{urban2016deep} use a small shallow net to mimick the output of a large deep net while using layer-wised distillation with $\ell_2$ loss of the outputs of student and teacher net. Similarly, Luo et al. \cite{luo2016face} use $\ell_2$ loss to train a compressed student model from the teacher model for face recognition. Gupta et al. \cite{Gupta_2016_CVPR} use supervision transfer to distil the knowledge from a trained CNN with unlabeled data or just a few labeled data. Lopez et. al \cite{lopez2015unifying} unifies the distillation and privileged information and  proposes a framework called Generalized distillation.