GD can be used in many scenarios such as multi-task learning, semi-supervised learning, and reinforcement learning. As generalized distillation only required for the training inputs $\{x_i,y_i\}_{i=1}^n$ and the output $s$ from the teacher function $f_t$, it can be naturally applied in domain adaptation, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (\textbf{GDSDA}), where the source model can be used as the teacher to output the soft labels and the student model is the target model. To be consistent with other works in domain adaptation, we use source model and target model to denote the teacher model and the student model in the rest of our paper respectively.

An important issue the extend GD to SDA is that, in Eq. \eqref{eq:distill}, each example is assigned a hard label (ground truth) and a soft label. However, in SDA, we are not able to obtain the hard label for unlabeled data. Here similar to GD \cite{lopez2015unifying}, we use the "fake label" strategy to the unlabeled data: for the labeled examples, we use \textit{one-hot} strategy to encode their labels while use gray code (all 0s) to the unlabeled examples as their label. Thus, each example in the target domain has its own label now. It is reasonable to argue that the "fake label" strategy would introduce extra noise and degrade the performance. However, we will show in our experiment that when the labeled examples are rare, we can still achieve improved performance by setting the proper value to the imitation parameter (See the single source experiment in Section \ref{sec:exp}).

Suppose we have $M-1$ source domains denoted as $D_s^{(j)}=\{X^{(j)},Y^{(j)}\}_{j=1}^{M-1}$ and the target domain $D_t=\{\{X,Y\}$ encoded with the "fake label" strategy. Similar to GD, the process of GDSDA is as follow:
\begin{enumerate}
    \item Train the source models $f^*_j$ for each of the $M-1$ domain with $\{X^{(j)},Y^{(j)}\}$.
    \item For each of the training example $x_i$ in the target domain, computer the corresponding soft label $y^*_{ij}$ with each of the source model $f^*_j$ and some temperature $T>0$.
    \item Learn the target model $f_t$ using the $(M+1)$-tuple $\{x_i,y_i,y^*_{i1},\dots,y^*_{M-1}\}_{i=1}^L$ with some imitation parameters $\{\lambda_i\}^M_{i=1}$ using \eqref{eq:GDDA_abs}:
\end{enumerate} 
\begin{equation}\label{eq:GDDA_abs}
\begin{aligned}
f_t(\lambda)=&\underset{f_t \in \mathcal{F}}{\arg \min}\frac{1}{L}\sum_{i=1}^{L}[\lambda_1\ell\left(y_i,f_t(x_i)\right)+\\&\sum_{j=1}^{M-1}\lambda_{j+1}\ell\left(y^*_j,f_t(x_i)\right)]\qquad
 \text{s.t.} \qquad \sum_i\lambda_i=1
\end{aligned}
\end{equation}
Compared to other works of SDA which require to utilize each example of the source domain, by either re-weighting \cite{Donahue_2013_CVPR,duan2012visual} or augmentation \cite{daume2010frustratingly}, GDSDA only requires the trained model from the source domain to generate the soft label. Considering the fact that it is more convenient to manipulate the source model than each of the examples in the source domain, GDSDA can be more effective than those previous method. For example, if we want to use ImageNet \cite{imagenet_cvpr09} as the source domain, it is almost impossible to access each of the millions of the examples while there are many well trained models publicly available online to be leveraged. Also, GDSDA is able to handle the multi-class scenario while some previous work, such as SHFA\cite{duan2012learning} can only solve the problem of binary class in SSDA. Moreover,it is clear that GDSDA is compatible with any type of source model that can output the soft label.

\subsection{Statistic analysis of GDSDA}
In this part, we provide statistic analysis for the scenarios where GDSDA would work. Before we provide the analysis, there are two basic assumptions for GDSDA to work well in domain adaptation: the \textit{assumption of distillation and the assumption of  transfer}.

\textbf{Assumption of Distillation:} The capacity (i.e. VC dimension) of target model $f_t$ is smaller than the capacity of source model $f^*$. This assumption is inherited from distillation.
\textbf{Assumption of Transfer:} The source model $f^*$ should work better than a target model $f'_t$ trained only with the hard labels. For example, when we only have a single labeled example for each class in the target training set, it is reasonable to assume that some source models trained from other domain could perform better than any model trained only with the target training data on the target task.

Let $\hat{R}(f)$ be the training error of the model $f$ with finite VC dimension $h$ on a training data with size $L$. According to ERM principle, the generalization error bound $R(f)$ of the model is:
\begin{equation}
R(f) \leq \hat{R}(f)+O\left(\sqrt{\frac{h}{L}}\right)
\end{equation}
As long as the target model can achieve similar training error on the target training data to the training error of the source model, i.e. $\hat{R}(f_t) \approx \hat{R}(f^*)$, it could have better generalization error than the source model, i.e. $R(f_t)\le R(f^*)$ \cite{hinton2015distilling}. 
%Previous work \cite{hinton2015distilling} suggests that if the target model can simulate the outputs of the source model, i.e. the soft label, well enough on the training data, the target model could outperform the source model on this task. 
It is worthy to notice that in this process, we don't require any labeled examples from the training set. This means GDSDA can effectively utilize the unlabeled data in SDA problem.

Arguably, the source model is biased on the target task due to the domain shift. More interestingly, as it is suggested in \cite{hinton2015distilling}, we can use some labeled data from the target domain to adjust the bias and achieve a better performance on the target task with Eq. \eqref{eq:distill}. This indicates that the target model trained with GDSDA can be further improved with the help of a few labeled data. Here, we use the imitation parameter $\lambda$ to control the importance of the soft label from the source model and the true label, i.e. the similarity between the source and target tasks. In addition, GDSDA can effectively utilize the unlabeled data to transfer the knowledge from source domain while using a few labeled data to compensate the domain shift (see the experiment results in Section \ref{sec:exp}).

\subsection{Key parameter: the imitation parameter}\label{sec:key}
From above we can see that GDSDA can effectively transfer the knowledge between source and target tasks. In this part, we theoretically demonstrate that the imitation parameter can greatly affect the performance of the target model.

In GDSDA, we have to decide the values of 2 parameters, the temperature $T$ and imitation parameter $\lambda$. The temperature $T$ control the smoothness of the soft label and the imitation parameter $\lambda$ controls how similar the target model is to the source model. Many previous works have addressed the importance of the similarity control when transferring the knowledge between different domains \cite{duan2012learning,duan2012visual}. Without carefully controlling the amount of the knowledge transferred from the source domain, it is easy for the target model to get degraded performance or even suffer from negative transfer \cite{pan2010survey}.
Here we show that the value of imitation parameter can greatly affect the performance of the target model. Specifically, we show that we should choose the imitation parameter that minimizes the empirical risk on the training data for distillation.

Let $f(x,\lambda)$ be a function with a finite VC dimension $h$ that minimizes the number of errors on the training data $\{x_i,y_i\}_{i=1}^L$. Let $v(\lambda)$ be the training error. Then according to the VC theory \cite{vapnik1999overview}, for an arbitrary loss function $\mathcal{L}(\cdot)$ we have the following bound:
\begin{equation}\label{eq:lambda_constraint}
P\left(\mathcal{L}\left(y,f(x,\lambda)\right)\geq 0\right) < v(\lambda)+O\left(\sqrt{\frac{h}{L}}\right)
\end{equation}
In other word, the optimal imitation parameter should be the one that can minimize the training error.  

In the previous work, the imitation parameter can only be determined by either brute force search \cite{lopez2015unifying} or background knowledge \cite{Tzeng_2015_ICCV} which greatly reduces its effectiveness. In domain adaptation, it is common that there could be multiple source models to be exploited. It is ideal to find a method that can determine the imitation parameter automatically.
