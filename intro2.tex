%An interesting regime for human learners is that it is often sufficient for people to grasp a new category with just a few example and generalize to the novel examples. In machine learning, this scenario is often referred to \textit{learning from sparse label data} (LSLD), such as \textit{one-shot learning scenario} \cite{hoffman2013one}. However, it is difficult for the machine learners to obtain this learning ability as it requires appropriately tuned inductive biases \cite{salakhutdinov2012one}. As a result, in transfer learning, the inductive biases are usually presented as the related source knowledge that can be transferred between tasks. There are many works on investigating transfer learning problem where the labeled data is extremely sparse especially in domain adaptation \cite{hoffman2013one}\cite{pfister2014domain}.
%How to effectively transfer the source knowledge for spares labeled data is still an interesting topic.

Domain adaptation can be used in many real applications, which addresses the problem of learning a target domain with the help of a different but related source domain. Previous methods show that carefully modeling the source data to compensate for the domain shift between different domains can significantly improve the performance on the target domain \cite{Donahue_2013_CVPR}. In real applications, it can be very expensive to obtain sufficient labeled examples while there are abundant unlabeled examples in the target domain. 
\textit{Semi-supervised domain adaptation} (SDA) tries to use some unlabeled examples as well as a few labeled ones from the target domain to compensate for the domain shift\cite{karl2001long}. Typically, the labeled examples are too few to construct a good classifier alone. How to effectively utilize the unlabeled examples is an important issue in SDA. 

Recently, a framework called \textit{Generalized Distillation} (\textbf{GD}) \cite{lopez2015unifying} was proposed, which allows the knowledge to be transferred between the teacher and student models effectively. GD can be considered as a hybrid framework of two popular paradigms, \textit{Distillation} \cite{hinton2015distilling} and \textit{Privileged Information} \cite{vapnik2015learning}. In GD, the student learner tries to distill the knowledge from the teacher model trained with the privileged information, and mimick the outputs of the teacher model on the training data. Remarkably, GD can be applied in many learning scenarios such as unsupervised, semi-supervised and multitask learning \cite{lopez2015unifying}. Given that GD has such ability in various learning scenarios, it is natural to ask the following two questions: (1) Can GD be applied to solve the SDA problem? (2) Is there any obstacle when we apply GD to real SDA applications?

To answer these two questions, in this paper, we first propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (\textbf{GDSDA}), and illustrate how it can solve the SDA problem. Secondly, we propose a novel algorithm GDSDA-SVM, that makes GDSDA more effective for real applications. Specifically, to answer the first question, we demonstrate that the target model trained with our GDSDA framework can effectively exploit the knowledge from the source domain and the unlabeled data of the target domain under the SDA setting. Specifically, we show that the knowledge transfer process is so effective that the target model can outperform the source model it learned from even without using any ground truth label. 
Different from many other paradigms which requires to access every single example of the source domain, GDSDA only requires the predicted class probabilities of the target domain examples from the source model. Therefore, GDSDA is more efficient especially when the source domain is relatively large and there is a well-trained source model.

Then we argue that the imitation parameter of GDSDA which controls the amount of knowledge transferred from the source can greatly affect the performance of the target model in prediction.
However, according to previous works \cite{lopez2015unifying,Tzeng_2015_ICCV}, the imitation parameter can only be determined by either brute force search or background knowledge. It would be ideal to find a method that can determine the imitation parameter automatically, especially when there are multiple source domains and imitation parameters.

Therefore, we propose a novel imitation parameter estimation method for GDSDA, called GDSDA-SVM that uses SVM as the base classifier and can determine the imitation parameter automatically. In particular, inspired by \cite{cawley2006leave}, we use mean square loss for GDSDA-SVM and show that the Leave-one-out cross validation (LOOCV) loss can be calculated in a closed form. By minimizing the LOOCV loss on the target training data, we can find the optimal imitation parameter for the target model. In our experiments, we show that GDSDA-SVM can effectively find the optimal imitation parameter and achieve competitive performance compared to methods using brutal force search. To summarize, the main contributions of this paper include: (1) We propose the framework GDSDA for domain adaptation and show that GDSDA can be used in many real SDA problems. (2) We propose the GDSDA-SVM that can effectively find the optimal imitation parameter for GDSDA.

%The rest of this paper is organized as follow: In Section \ref{sec:work}, we describe the related work on privileged information and distillation in domain adaptation. Section \ref{sec:gdda} we propose our framework of GDSDA and provide some statistic analysis. Based on that, we propose our GDSDA-SVM in Section \ref{sec:svm}. Experimental results are shown in Section \ref{sec:exp}. Some discussion and conclusion are provided in Section \ref{sec:con}.