%An interesting regime for human learners is that it is often sufficient for people to grasp a new category with just a few example and generalize to the novel examples. In machine learning, this scenario is often referred to \textit{learning from sparse label data} (LSLD), such as \textit{one-shot learning scenario} \cite{hoffman2013one}. However, it is difficult for the machine learners to obtain this learning ability as it requires appropriately tuned inductive biases \cite{salakhutdinov2012one}. As a result, in transfer learning, the inductive biases are usually presented as the related source knowledge that can be transferred between tasks. There are many works on investigating transfer learning problem where the labeled data is extremely sparse especially in domain adaptation \cite{hoffman2013one}\cite{pfister2014domain}.
%How to effectively transfer the source knowledge for spares labeled data is still an interesting topic.

Domain adaptation can be used in many real applications, which assumes that the data distribution in the target domain differs from the data distribution in the source domain. Previous methods show that carefully modeling the source data to compensate the domain shift between different domains can significantly improve the performance on the target domain \cite{Donahue_2013_CVPR}. In real applications, it is often very expensive to obtain sufficient labeled examples while there are abundant unlabeled examples in the target domain. 
\textit{Semi-supervised domain adaptation} (SDA) tries to utilize some unlabeled data from the target domain to compensate the domain shift as well as a few labeled data \cite{karl2001long}. Typically, the labeled data are too few to  construct a good classifier alone. How to effectively utilize the unlabeled data is an important issue in SSDA. 

Recently, a framework called \textit{Generalized Distillation} (\textbf{GD}) \cite{lopez2015unifying} was proposed, which allows the knowledge to be transferred between the teacher and student models effectively. GD can be considered as a hybrid framework of two popular paradigms, \textit{Distillation} \cite{hinton2015distilling} and \textit{privileged information} \cite{vapnik2015learning}. In GD, the student learner tries to distill the knowledge of teacher model trained with the privileged information, and mimick the outputs of the teacher model on the training data. Remarkably, GD can be applied in many learning scenarios such as unsupervised, semi-supervised and multitask learning \cite{lopez2015unifying}. Given that GD has such ability in various learning scenarios, people would ask the following two questions: (1) Can GD be applied to solve the SDA problem? (2) Is there any obstacle when we apply GDSDA for real applications?

To answer these two questions, in this paper, we first propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (\textbf{GDSDA}), and illustrate how it can solve the SDA problem. Secondly, we propose a novel algorithm GDSDA-SVM, that makes GDSDA more effective for practical applications. Specifically, to answer the first question, we show that the machine learner trained with our GDSDA framework can effectively exploit the knowledge from the source domain and outperform the source model it learned from when labeled data is sparse.
Different from many other paradigms where the source knowledge is required in both training and testing process, such as \cite{kuzborskij2013stability}, the source knowledge is only required in the training process of GDSDA and the machine learner can work on itself along when testing. More interestingly, we demonstrate that GDSDA can utilize the knowledge from most of the existing classifier.

Then we argue that the imitation parameter of GDSDA controls the amount of knowledge transferred from the source which can greatly affect the performance of the machine learner in real applications (see Section \ref{sec:gdda}).
However, according to previous work \cite{lopez2015unifying,Tzeng_2015_ICCV}, the imitation parameter can only be determined by either brute force search or background knowledge. It is ideal to find a method that can determine the imitation parameter automatically, especially when there are more than one imitation parameter to be determined.

Therefore, we propose a novel imitation parameter estimation method for GDSDA, called GDSDA-SVM that uses SVM as the base learner and can determine the imitation parameter automatically. In particular, inspired by \cite{cawley2006leave}, we use $\ell_2-$loss for GDSDA-SVM and show that the Leave-one-out cross validation (LOOCV) loss can be calculated in a closed form. By minimizing the LOOCV loss on the target training data, we can find the optimal imitation parameter for the target model. In our experiments, we show that GDSDA-SVM can effectively find the optimal imitation parameter and achieve competitive performance compared to methods using brutal force search. In addition, the main contributions of this paper include: (1) We propose the framework GDSDA for domain adaptation and show that GDSDA can be used in many domain adaptation problems. (2) We propose the GDSDA-SVM that can effectively find the optimal imitation parameter for GDSDA.

%The rest of this paper is organized as follow: In Section \ref{sec:work}, we describe the related work on privileged information and distillation in domain adaptation. Section \ref{sec:gdda} we propose our framework of GDSDA and provide some statistic analysis. Based on that, we propose our GDSDA-SVM in Section \ref{sec:svm}. Experimental results are shown in Section \ref{sec:exp}. Some discussion and conclusion are provided in Section \ref{sec:con}.